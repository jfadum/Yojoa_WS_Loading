---
title: "Watershed Loading Calcs"
author: "M. Ross and J Fadum"
date: "2023-04-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## DISCHARGE

```{r include=FALSE}

#PACKAGES
library(tidyverse)
library(signal)
library(xts)
library(dygraphs) 
library(RcppRoll)
library(imputeTS)
library(ggthemes)
library(broom)
library(tsibble)
library(GGally)
library(tidyr)
library(lubridate)
library(readr)
library(plotly) 
library(ggthemes)


set.seed(37)

#Raices #8C510A
#Helado #BF812D
#Varsovia  #01665E
#Yure    #542788 
#Balas  #35978F
#Cacao #8073AC
```


Here we load the data from the pressure transducers and identify any jumps. 

```{r  warning=FALSE, error=FALSE, message=FALSE}
#Load and trim data----

##This is the data from the pressure transducers 
raw_cm <- read_csv('sensor_2018_2019.csv') %>%
  mutate(datetime = round_date(mdy_hm(datetime),'15 mins')) %>% 
  select(Event, site, datetime, stage, temp)
  

#Find data jumps----

## Jumps are when water at high velocity discharges into a zone of lower velocity. 
## The rapidly flowing water is abruptly slowed while stage increases

jumps <- raw_cm %>%
  group_by(site) %>%
  arrange(site,datetime) %>%
  mutate(lag_event = lag(Event), #creates lagged version
         download = ifelse((Event - lag_event) >= 1 | is.na(lag_event), 0,1),
         session = cumsum(download)) 

#Make a dataframe of the first and last X of measurements for each session. X is the of measurements to average over. 


jump_hunter <- function(df = jumps, x = 10){
  last_x <- df %>%
    group_by(site,session) %>%
    # Get the last X obs per site and session
    slice_tail(n = x) %>%
    #Take median stage and lasttime recorded
    summarize(last = median(stage,na.rm=T),
              lasttime = max(datetime)) 
  
#Same as above but for the first X slice of data
  first_x <- jumps %>%
    group_by(site,session) %>%
    slice_head(n = x) %>%
    summarize(first = median(stage,na.rm=T),
              firsttime = min(datetime))
  
#Join these datasets and find the median stage difference and the time diff. The time diff is critical because with big time gaps the jumps are likely wrong. 
  
  plunge <- inner_join(first_x,last_x) %>%
    group_by(site) %>%
    mutate(lead_first = lead(first,1,last[n()]),
           event_jump =  last-lead_first,
           cume_jump = rev(cumsum(rev(event_jump))),
           timediff = lasttime - lead(firsttime,1,lasttime[n()])) %>%
    select(site,session,cume_jump,lasttime)
  
  
  return(plunge)
}


jumped <- jumps %>%
  inner_join(jump_hunter(jumps)) %>%
  #Varsovia shouldn't jump so removed jumps here 
  #Attached to concrete channel in horizontal setting. 
  mutate(cume_jump = ifelse(site == 'varsovia',0, cume_jump)) %>%
  mutate(jumped_stage = stage - cume_jump)


 t.xts <- jumped %>%
   ungroup() %>%
   dplyr::filter(site == 'balas') %>%
   dplyr::select(stage,jumped_stage,datetime) %>%
   xts(. %>% select(-datetime),order.by=.$datetime)


 dygraph(t.xts) %>%
   dyOptions(useDataTimezone = T)
 
  t.xts <- jumped %>%
   ungroup() %>%
   dplyr::filter(site == 'varsovia') %>%
   dplyr::select(stage,jumped_stage,datetime) %>%
   xts(. %>% select(-datetime),order.by=.$datetime)


 dygraph(t.xts) %>%
   dyOptions(useDataTimezone = T)
 
  t.xts <- jumped %>%
   ungroup() %>%
   dplyr::filter(site == 'helado') %>%
   dplyr::select(stage,jumped_stage,datetime) %>%
   xts(. %>% select(-datetime),order.by=.$datetime)


 dygraph(t.xts) %>%
   dyOptions(useDataTimezone = T)
 
  t.xts <- jumped %>%
   ungroup() %>%
   dplyr::filter(site == 'raices') %>%
   dplyr::select(stage,jumped_stage,datetime) %>%
   xts(. %>% select(-datetime),order.by=.$datetime)


 dygraph(t.xts) %>%
   dyOptions(useDataTimezone = T)
```

Next we compare the sensors data and weekly reference measurements (different than the rating curves which we will load later)

```{r warning=FALSE, error=FALSE, message=FALSE}
#In field observations
staff <- read_csv('Staff_TS_27MAY21.csv') %>% 
  select(1:4) %>%
  mutate(datetime = mdy_hm(paste(date,time))) %>%
  dplyr::filter(!is.na(datetime)) %>%
  mutate(datetime = round_date(datetime,'15 mins'))

##attach to data from transducers
staff_sensor <- staff %>%
  inner_join(jumped,by = c('site','datetime')) %>%
  mutate(month = month(datetime))

#April onward comparison of staff vs. sensor----
##Our data quality improves after April 2019 so we will be using April 2019-April 2020 as our '1 calendar year'

start_date = mdy_hms('04/01/2019 00:00:00')
staff_sensor_april <- staff_sensor %>%
  dplyr::filter(datetime > start_date)

stage_v_sensor<- ggplot(staff_sensor_april,aes(x=staff_stage,y=jumped_stage,color = month)) + 
  geom_point() + 
  facet_wrap(~site,scales='free') + 
  stat_smooth(method = 'lm') + 
  labs(title= "After April 2019")+
  scale_color_viridis_c() + 
  xlab('Regla measurement') + 
  ylab('Sensor stage (jumped)')+
  theme_few()



# Sensor to staff conversions----

#creates a linear model using transducer data
staff_conversion <- staff_sensor_april %>%
  group_by(site) %>%
  nest() %>%
  mutate(mods = map(data,~lm(staff_stage ~ jumped_stage, data = .x)),
         #Model summaries
         mod_sum = map(mods,glance),
         mod_tidy = map(mods,tidy))

staff_summaries <- staff_conversion %>%
  unnest(mod_sum) %>%
  select(site,r.squared,p.value)

knitr::kable(staff_summaries)


```

The outputs of this models are predictions where jumped stage is now converted to staff stage at the 0.05,0.5, and 0.95 confidence intervals. It will give us a sense of how uncertain our Q estimates are given our uncertainty in our staff relationships.

```{r  warning=FALSE, error=FALSE,  message=FALSE}


my.predict <- function(mods,sensor_data){
  out <- predict.lm(mods,sensor_data,interval='confidence')
  return(tibble(med_staff = out[,1] %>% as.numeric(.),
                min_staff = out[,2] %>% as.numeric(.),
                max_staff = out[,3] %>% as.numeric(.)))
}

jumped_staff <- jumped %>%
  group_by(site) %>%
  nest() %>%
  rename(sensor_data = data) %>%
  inner_join(staff_conversion,by = 'site') %>%
  mutate(preds = map2(mods,sensor_data,my.predict)) %>%
  select(-mods,-mod_sum,-mod_tidy,-data) %>%
  unnest(c(sensor_data,preds)) 

```

Next we load the ratings curves. See text for rating curve methods 

```{r  warning=FALSE, error=FALSE,  message=FALSE}


q <- read_csv('rating_curves_2018_2019.csv')  %>%  
  mutate(manual_stage = manual_stage * 100)

rating_curves<- ggplot(q,aes(x=manual_stage,y=q)) + 
  geom_point() + 
  facet_wrap(~site, scales = 'free') +
  theme_few()

rating_curves

```

Finally, we model discharge using the jumped staff dataframe and our rating curves. 
Note: This is the method used to determine daily discharge in Raices, Helado, Balas and Varsovia. The remaining two tributaries (Cacao and Yure) used manning's equation. 

```{r  warning=FALSE, error=FALSE,  message=FALSE}
## Likely shapes (assign log to natural sites and linear to channel)
functional_response <- tibble(site = c('balas','helado','raices','varsovia'),
                              response = c('log','log',
                                           'log','linear'))

#linear model for varsovia and log model for other 3 sites
man_q <- function(df){
  if(df$response[1] == 'log'){
    mod <- lm(log(q) ~ manual_stage, data = df)
  } else{
    mod <- lm(q ~ manual_stage, data = df)
  }
}

q_mods <- q %>%
  inner_join(functional_response) %>%
  group_by(site) %>%
  nest() %>%
  mutate(q_mods = map(data,man_q),
         #Model summaries
         q_sum = map(q_mods,glance),
         q_tidy = map(q_mods,tidy)) %>%
  inner_join(functional_response) 


q_summaries <- q_mods %>%
  unnest(q_sum) %>%
  select(site,r.squared,p.value,response)

knitr::kable(q_summaries)

q.predict <- function(mods,sensor_data){
  out <- predict.lm(mods,sensor_data,interval='confidence')
  return(tibble(med_q = out[,1] %>% as.numeric(.),
                min_q = out[,2] %>% as.numeric(.),
                max_q = out[,3] %>% as.numeric(.)))
  
}



q_preds <- jumped_staff %>%
  rename(manual_stage = jumped_stage) %>%
  group_by(site) %>% 
  nest() %>%
  rename(sensor_data = data) %>%
  inner_join(q_mods,by = 'site') %>%
  mutate(preds = map2(q_mods,sensor_data,q.predict)) %>%
  select(-q_mods,-q_sum,-q_tidy,-data) %>%
  unnest(c(sensor_data,preds)) %>%
  mutate(across(ends_with('_q'), ~ ifelse(response == 'log',exp(.),.))) %>%
  dplyr::filter(datetime > mdy_hms('04/01/2019 00:00:00'),
                across(ends_with('_q'), ~ . < 10)) %>%
  mutate(date = as.Date(datetime)) 



```


## LOADING

```{r include=FALSE}

#PACKAGES
library(tidyverse)
library(signal)
library(xts)
library(dygraphs) 
library(RcppRoll)
library(imputeTS)
library(ggthemes)
library(broom)
library(tsibble)
library(GGally)
library(tidyr)
library(lubridate)
library(readr)
library(plotly) 
library(ggthemes)
library(dplyr)


set.seed(37)
#Figure colors...


```

Now we will be interpolating nutrient concentrations to get daily loading where possible (i.e. the tributaries with pressure transducers)

A note about joining the data sets... Since the rivers behave in a largely chemostatic manner, we can't build models to go from Q to 15 min predictions of concentration (at least not reliably). Therefore, we are using Liken's method (we will assume the concentration linearly drifts between samples and then just sum the concentrations using that assumption)




```{r warning=FALSE, error=FALSE, message=FALSE}
nutes <- read_csv('13OCT21_Rivers.csv') %>% 
  mutate(datetime = mdy_hms(paste(date,'12:00:00')),
         date = as.Date(datetime),
         tp = as.numeric(tp))

nh4<- nutes %>% 
  select(date, site, nh4_n, datetime) %>% 
  group_by(date, site) %>% 
  mutate(nh4_n= mean(nh4_n)) %>% 
  unique()

no3<- nutes %>% 
  select(date, site, no3_n, datetime) %>% 
  group_by(date, site) %>% 
  mutate(no3_n= mean(no3_n)) %>% 
  unique()  
  
tp<- nutes %>% 
  select(date, site, tp, datetime) %>% 
  group_by(date, site) %>% 
  mutate(tp= mean(tp)) %>% 
  unique() 

doc<- nutes %>% 
  select(date, site, doc, datetime) %>% 
  group_by(date, site) %>% 
  mutate(doc= mean(doc)) %>% 
  unique() 
         
tdn<- nutes %>% 
  select(date, site, tdn, datetime) %>% 
  group_by(date, site) %>% 
  mutate(tdn= mean(tdn)) %>% 
  unique() 
        
nutes<-cbind(nh4, no3[,3], tp[,3], doc[,3], tdn[,3])
#now that we only have one observation per day we can join ...


q_preds <- jumped_staff %>%
  rename(manual_stage = jumped_stage) %>%
  group_by(site) %>% 
  nest() %>%
  rename(sensor_data = data) %>%
  inner_join(q_mods,by = 'site') %>%
  mutate(preds = map2(q_mods,sensor_data,q.predict)) %>%
  select(-q_mods,-q_sum,-q_tidy,-data) %>%
  unnest(c(sensor_data,preds)) %>%
  mutate(across(ends_with('_q'), ~ ifelse(response == 'log',exp(.),.))) %>%
  dplyr::filter(datetime > mdy_hms('04/01/2019 00:00:00'),
                across(ends_with('_q'), ~ . < 10)) %>%
  mutate(date = as.Date(datetime)) 

sensor_qs <- q_preds %>%
  as_tibble() %>%
  group_by(date,site) %>%
  summarize(q = mean(med_q,na.rm=T))

sensor_qs<- as.data.frame(sensor_qs)
cacao_q<- read_csv("q_cacao_2019.csv")
cacao_q$date<- mdy(cacao_q$date)
cacao_q<- cacao_q %>% 
  mutate(site= "cacao") %>% 
  select(date, site, q) %>% 
  drop_na()

yure_q<- read_csv("q_yure_2019.csv")
yure_q$date<- mdy(yure_q$date)
yure_q<- yure_q %>% 
  mutate(site= "yure") %>% #add site so can combine data sets
drop_na()


daily_q<- rbind(sensor_qs, cacao_q, yure_q)

complete <- daily_q %>%
  full_join(nutes %>% select(-site,-datetime)) %>%
  group_by(site) %>%
  mutate(interpolated = ifelse(is.na(doc),'Yes','No')) %>%
  mutate(across(c('nh4_n','no3_n','tp','doc','tdn'),
          ~na_interpolation(.))) %>%
  arrange(site,date) %>%
  dplyr::filter(date < ymd('2020-04-01'),
                date > ymd('2019-04-01')) %>% 
  filter(site != "Canal")


complete_long <- complete %>%
  pivot_longer(cols = nh4_n:tdn) %>% 
  mutate(month= month(date)) 

fluxes <- complete_long%>%
  #Convert Q from m3s to lpd
  mutate(q_lpd = q*1000*60*60*24) %>%
  #Convert nutes from mg/L to kg/l
  mutate(kgl = value/(1000*1000)) %>%
  #Convert to daily flux
  mutate(kg_day = q_lpd*kgl) %>% 
  drop_na()


```


## Figure 5a

```{r warning=FALSE, error=FALSE, message=FALSE}
tdn_daily<- fluxes %>% 
  filter(name=="tdn")

daily_tdn<- ggplot(tdn_daily, aes(x = date, y = kg_day, color=site, shape= interpolated)) + 
  geom_point(size= 5)+
#  geom_line(alpha= 0.5)+
  ggtitle("Tributary Discharge")+
  ylab("TDN (kg/day)")+
  scale_color_manual(values=c("#35978F", "#8073AC", "#BF812D", "#8C510A","#01665E", "#542788"))+
  scale_shape_manual(values= c(19,1))+
  theme_few()+
  facet_wrap(~site , ncol= 3)+
    scale_x_date(limits = as.Date(c("2019-04-01", "2020-04-01")))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  theme(legend.position = "none")

daily_tdn

tp_daily<- fluxes %>% 
  filter(name=="tp")

daily_tp<- ggplot(tp_daily, aes(x = date, y = kg_day, color=site, shape= interpolated)) + 
  geom_point(size= 5)+
#  geom_line(alpha= 0.5)+
  ggtitle("Tributary Discharge")+
  ylab("TP (kg/day)")+
  scale_color_manual(values=c("#35978F", "#8073AC", "#BF812D", "#8C510A","#01665E", "#542788"))+
  scale_shape_manual(values= c(19,1))+
  theme_few()+
  facet_wrap(~site , ncol= 3)+
    scale_x_date(limits = as.Date(c("2019-04-01", "2020-04-01")))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  theme(legend.position = "none")

daily_tp



count<- tp_daily %>% 
  mutate(obs= 1) %>% 
  group_by(site) %>% 
  summarise(count= sum(obs))
```


## Figure 5b

```{r warning=FALSE, error=FALSE, message=FALSE}

data<- fluxes %>%   #create new dataframe to easily trim dates and sum by month
#  subset(date> "2019-04-01" & date < "2020-04-01") %>% 
  mutate(month=month(date))  %>% 
  select(site, name, kg_day, month) %>% 
  group_by(site,name, month) %>% 
  summarise(value=sum(kg_day)) 



data$month<- as.character(data$month)
data$month <- factor(data$month, levels = c("1", "2", "3","4", "5", "6", "7", "8", "9", "10", "11", "12"))


tdn <- data %>% 
  dplyr::filter(name =="tdn") %>% 
  group_by(site, month) %>% 
  summarise(value=sum(value)) 


tdn_stack<-ggplot(tdn, aes(fill=site, y=value, x=month)) +
  geom_bar(position="stack", stat="identity")+
   ggtitle("") +
  theme_few()+
  ylab("TDN (kg)")+
  xlab("")+
    theme(legend.position = "none")+
  scale_fill_manual(values=c("#35978F", "#8073AC", "#BF812D", "#8C510A","#01665E", "#542788"))
  

tdn_stack

tp <- data %>% 
  dplyr::filter(name =="tp") %>% 
  group_by(site, month) %>% 
  summarise(value=sum(value)) 

tp_stack<- ggplot(tp, aes(fill=site, y=value, x=month)) +
  geom_bar(position="stack", stat="identity")+
  ylab("TP (kg)")+
  xlab("")+
  theme_few()+
  theme(legend.position = "none")+
  scale_fill_manual(values=c("#35978F", "#8073AC", "#BF812D", "#8C510A","#01665E", "#542788"))
tp_stack



```

## Determine "Normalization" for missing days

```{r warning=FALSE, error=FALSE, message=FALSE}
tdn_daily %>% 
  mutate(count= 1) %>% 
  group_by(site) %>% 
  summarise(count=sum(count)) %>% 
  mutate(norm= count/365)

tp_daily %>% 
  mutate(count= 1) %>% 
  group_by(site) %>% 
  summarise(count=sum(count))%>% 
  mutate(norm= count/365)
```


## Figure 3c

```{r warning=FALSE, error=FALSE, message=FALSE}
N_P_aqua <- read_csv("N_P_aqua.csv")

N_aqua <- N_P_aqua %>% 
  rename(month= mes_2013,
         value= N_kg) %>% 
  mutate(site= "Aquaculture") %>% 
  select(site, value ) %>% 
  group_by(site) %>% 
  mutate(value=sum(value)) %>% 
  unique()

tdn <- data %>% 
  dplyr::filter(name =="tdn") %>% 
  group_by(site) %>% 
  summarise(value=sum(value)) %>% 
  mutate(norm= c(1,.131,1,1,.934,.126),
         max_estimate= value/norm,
         dif= max_estimate-value) 

underestimate<- tdn %>% 
  mutate(value=sum(tdn$dif)) %>% 
  select(value) %>% 
  unique() %>% 
  mutate(site= "X estimate") %>% 
  select(site, value)
  

tdn<- tdn %>% 
  select(site, value)

N_loading<- rbind(tdn, N_aqua, underestimate)

N_loading<- N_loading%>% 
  arrange(desc(site)) %>%
  mutate(prop = value / sum(N_loading$value) *100) %>%
  mutate(ypos = cumsum(prop)- 0.5*prop )



N_pie<-ggplot(N_loading, aes(x="", y=value, fill=site)) +
  geom_bar(stat="identity", width=1, color="white") +
  coord_polar("y", start=0) +
  scale_fill_manual(values=c("red4","#35978F", "#8073AC", "#BF812D", "#8C510A","#01665E", "antiquewhite","#542788"))+
  theme_void()+
    theme(legend.position = "none")

N_pie


P_aqua <- N_P_aqua %>% 
  rename(month= mes_2013,
         value= P_kg) %>% 
  mutate(site= "Aquaculture") %>% 
  select(site, value ) %>% 
    group_by(site) %>% 
  mutate(value=sum(value))%>% 
  unique()

tp <- data %>% 
  dplyr::filter(name =="tp") %>% 
  group_by(site) %>% 
  summarise(value=sum(value)) %>% 
  mutate(norm= c(1,.131,1,1,.934,.126),
         max_estimate= value/norm,
         dif= max_estimate-value) 

underestimate<- tp %>% 
  mutate(value=sum(tp$dif)) %>% 
  select(value) %>% 
  unique() %>% 
  mutate(site= "X estimate") %>% 
  select(site, value)
  

tp<- tp %>% 
  select(site, value)

P_loading<- rbind(tp, P_aqua, underestimate)

P_loading<- P_loading%>% 
  arrange(desc(site)) %>%
  mutate(prop = value / sum(P_loading$value) *100) %>%
  mutate(ypos = cumsum(prop)- 0.5*prop )



P_pie<-ggplot(P_loading, aes(x="", y=value, fill=site)) +
  geom_bar(stat="identity", width=1, color="white") +
  coord_polar("y", start=0) +
  scale_fill_manual(values=c("red4","#35978F", "#8073AC", "#BF812D", "#8C510A","#01665E", "antiquewhite","#542788"))+
  theme_void()+
    theme(legend.position = "none")

P_pie

#ggsave("N_pie.png", plot = N_pie, width = 15, height = 15, units = "in", dpi= 600)
#ggsave("P_pie.png", plot = P_pie, width = 15, height = 15, units = "in", dpi= 600)
```
## Aditional analysis: Loading per km2

```{r}
library(readxl)
area<- read_excel("drainage_areas.xlsx")

calc_P<- left_join(area, P_loading, by= "site") %>% 
  mutate(P_per_area= value/area_km)

calc_N<- left_join(area, N_loading, by= "site") %>% 
  mutate(P_per_area= value/area_km)
```



